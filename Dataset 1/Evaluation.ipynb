{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation metrics of the trained model from resnet18_dataset_1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### PRECISION ######\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import precision_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'model' is your trained model and 'test_loader' is your DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # Collect the true labels and predictions for precision calculation\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate precision\n",
    "# Note: Set average='macro' for multi-class classification or 'binary' for binary classification\n",
    "precision = precision_score(true_labels, predictions, average='macro')  # Change 'macro' as needed\n",
    "\n",
    "print(f\"Precision: {precision}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### ROC ######\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import interp\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "# Assuming 'model' is your trained model and 'test_loader' is your DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "n_classes = 3  # Change this to the number of classes in your dataset\n",
    "\n",
    "# Prepare true labels and predictions\n",
    "true_labels = []\n",
    "pred_probs = []\n",
    "\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        # Convert outputs to probabilities\n",
    "        probs = softmax(outputs, dim=1)\n",
    "\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    pred_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "# Binarize the labels for multi-class ROC\n",
    "true_labels = label_binarize(true_labels, classes=[*range(n_classes)])\n",
    "# pred_probs is already in the correct format\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(true_labels[:, i], pred_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curve for each class\n",
    "colors = cycle(['blue', 'red', 'green', 'cyan', 'magenta', 'yellow'])\n",
    "for i, color in zip(range(n_classes), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) - Multi-Class')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### RECALL ######\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import recall_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'model' is your trained model and 'test_loader' is your DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # Collect true labels and predictions\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate Recall\n",
    "# Note: Specify the averaging method as 'macro' for multi-class problems, or adjust as needed\n",
    "recall = recall_score(true_labels, predictions, average='macro')  # Change 'macro' to 'binary' if binary classification\n",
    "\n",
    "print(f\"Recall: {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### F1 ######\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'model' is your trained model and 'test_loader' is your DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # Collect true labels and predictions\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate F1 Score\n",
    "# Note: Specify the averaging method as 'macro' for multi-class problems, or adjust as needed\n",
    "f1 = f1_score(true_labels, predictions, average='macro')  # Change 'macro' as necessary\n",
    "\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### CONFUSION MATRIX ######\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assuming 'model' is your trained model and 'test_loader' is your DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for inputs, labels in test_loader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    # Collect true labels and predictions\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "    predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# Optionally, visualize the Confusion Matrix\n",
    "fig, ax = plt.subplots(figsize=(10,10))  # Adjust the size as needed\n",
    "sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')  # fmt='d' for integer decoration\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
